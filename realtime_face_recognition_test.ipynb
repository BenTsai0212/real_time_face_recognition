{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ac08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn_cv2 import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from face_recognition.face_recognition import Face_recognition\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb66b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_pth = './test'\n",
    "detected_img_pth = './detected_img'\n",
    "detector = MTCNN()\n",
    "face_model = Face_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6660f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affineMatrix(lmks, scale=4.5):\n",
    "    nose = np.array(lmks['nose'], dtype=np.float32)\n",
    "    left_eye = np.array(lmks['left_eye'], dtype=np.float32)\n",
    "    right_eye = np.array(lmks['right_eye'], dtype=np.float32)\n",
    "    eye_width = right_eye - left_eye\n",
    "    angle = np.arctan2(eye_width[1], eye_width[0])\n",
    "    center = nose\n",
    "    alpha = np.cos(angle)\n",
    "    beta = np.sin(angle)\n",
    "    w = np.sqrt(np.sum(eye_width**2)) * scale\n",
    "    m = [[alpha, beta, -alpha * center[0] - beta * center[1] + w * 0.5],\n",
    "        [-beta, alpha, beta * center[0] - alpha * center[1] + w * 0.5]]\n",
    "    return np.array(m), (int(w), int(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359355a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "362c6473",
   "metadata": {},
   "source": [
    "# mtcnn-cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e62fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: daoko, score: 47.39771792382905\n",
      "predict: daoko, score: 48.45799785671812\n",
      "predict: khaby, score: 45.85431344581374\n",
      "predict: rock, score: 43.61400243007776\n",
      "predict: daoko, score: 43.31076109048092\n",
      "predict: trump, score: 44.379658410043426\n",
      "predict: billie, score: 44.11426060127489\n",
      "predict: khaby, score: 46.21105988820394\n",
      "predict: daoko, score: 53.55472239581022\n",
      "predict: musk, score: 40.74860847357547\n"
     ]
    }
   ],
   "source": [
    "target = 'nba_many_players.jpeg'\n",
    "\n",
    "img = cv2.imread(os.path.join(test_img_pth, target))\n",
    "detected_faces = detector.detect_faces(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# open cv2_window\n",
    "cv2.namedWindow(\"image\", cv2.WINDOW_NORMAL) \n",
    "\n",
    "for cnt, face in enumerate(detected_faces):\n",
    "    if face['confidence'] < 0.85: continue    \n",
    "    mat, size = affineMatrix(face['keypoints'])\n",
    "    \n",
    "    # get cropped image to recognition\n",
    "    result = cv2.warpAffine(img, mat, size)\n",
    "    result = Image.fromarray(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    predict = face_model.predict(result, is_cropped=True)\n",
    "    if predict == None: continue\n",
    "    pred_label, _, score = predict\n",
    "    print(f'predict: {pred_label}, score: {score}')\n",
    "    \n",
    "    # save cropped face image to ./detected_img\n",
    "    cv2.imwrite(os.path.join(detected_img_pth, str(cnt) + '_' +target), cv2.warpAffine(img, mat, size))\n",
    "    \n",
    "    # draw face border\n",
    "    face_pos = face['box']\n",
    "    cv2.rectangle(img, (face_pos[0], face_pos[1]), (face_pos[0]+face_pos[2],\n",
    "                                                    face_pos[1]+face_pos[3]), (0, 255, 0), 3)\n",
    "    \n",
    "    # output prediction on image\n",
    "    cv2.putText(img, f'{pred_label}, score:{int(score)}', (face_pos[0], face_pos[1]), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('image')\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3fbe52",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1588, 1, 67, 85]\n",
      "0.9999768733978271\n",
      "{'left_eye': (1606, 36), 'right_eye': (1639, 36), 'nose': (1620, 53), 'mouth_left': (1607, 68), 'mouth_right': (1635, 69)}\n",
      "[1863, 22, 73, 82]\n",
      "0.9999547004699707\n",
      "{'left_eye': (1880, 53), 'right_eye': (1912, 52), 'nose': (1891, 74), 'mouth_left': (1881, 87), 'mouth_right': (1907, 85)}\n",
      "[762, 391, 86, 106]\n",
      "0.9998884201049805\n",
      "{'left_eye': (807, 430), 'right_eye': (838, 432), 'nose': (830, 456), 'mouth_left': (804, 475), 'mouth_right': (832, 475)}\n",
      "[825, 333, 61, 85]\n",
      "0.9998205304145813\n",
      "{'left_eye': (842, 365), 'right_eye': (870, 364), 'nose': (856, 379), 'mouth_left': (844, 398), 'mouth_right': (865, 398)}\n",
      "[937, 107, 87, 113]\n",
      "0.9998030066490173\n",
      "{'left_eye': (964, 156), 'right_eye': (1002, 157), 'nose': (983, 184), 'mouth_left': (966, 196), 'mouth_right': (997, 197)}\n",
      "[192, 64, 67, 93]\n",
      "0.9997523427009583\n",
      "{'left_eye': (212, 98), 'right_eye': (241, 107), 'nose': (216, 119), 'mouth_left': (202, 133), 'mouth_right': (226, 140)}\n",
      "[1149, 0, 72, 72]\n",
      "0.9997192025184631\n",
      "{'left_eye': (1176, 17), 'right_eye': (1209, 17), 'nose': (1195, 40), 'mouth_left': (1178, 52), 'mouth_right': (1204, 52)}\n",
      "[513, 165, 88, 114]\n",
      "0.9990779161453247\n",
      "{'left_eye': (555, 212), 'right_eye': (590, 207), 'nose': (581, 234), 'mouth_left': (559, 254), 'mouth_right': (587, 250)}\n",
      "[1508, 235, 84, 110]\n",
      "0.9853611588478088\n",
      "{'left_eye': (1533, 274), 'right_eye': (1573, 278), 'nose': (1554, 294), 'mouth_left': (1534, 310), 'mouth_right': (1567, 314)}\n",
      "[1271, 612, 59, 95]\n",
      "0.8998157382011414\n",
      "{'left_eye': (1294, 648), 'right_eye': (1306, 652), 'nose': (1292, 665), 'mouth_left': (1293, 681), 'mouth_right': (1301, 685)}\n",
      "[1327, 621, 73, 99]\n",
      "0.7740508913993835\n",
      "{'left_eye': (1350, 658), 'right_eye': (1368, 643), 'nose': (1363, 670), 'mouth_left': (1365, 698), 'mouth_right': (1378, 688)}\n"
     ]
    }
   ],
   "source": [
    "for face in detected_faces:\n",
    "    print(face['box'])\n",
    "    print(face['confidence'])\n",
    "    print(face['keypoints'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b6059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "802baf67",
   "metadata": {},
   "source": [
    "# Video detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cb10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'khaby.mp4'\n",
    "cap = cv2.VideoCapture(os.path.join(test_img_pth, target))\n",
    "frame_rate = 90\n",
    "process_rate = 30\n",
    "base_detection_confidence = 0.95\n",
    "base_recognition_confidence = 55\n",
    "prev = 0\n",
    "process_prev = 0\n",
    "face_model.test_mode = False\n",
    "\n",
    "# start video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # video end, exiting\n",
    "    if not ret:\n",
    "        break \n",
    "    \n",
    "    time_elapsed = time.time() - prev\n",
    "    process_time_epls = time.time() - process_prev\n",
    "    \n",
    "    # set frame process rate\n",
    "    if process_time_epls > 1./process_rate:\n",
    "        process_prev = time.time()\n",
    "    \n",
    "        # frame process\n",
    "        detected_faces = detector.detect_faces(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        for cnt, face in enumerate(detected_faces):\n",
    "            if len(detected_faces) == 0: break\n",
    "            \n",
    "            if face['confidence'] < base_detection_confidence: continue    \n",
    "            mat, size = affineMatrix(face['keypoints'])\n",
    "\n",
    "            # get cropped image to recognition\n",
    "            result = cv2.warpAffine(frame, mat, size)\n",
    "            result = Image.fromarray(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "            predict = face_model.predict(result, is_cropped=True)\n",
    "            \n",
    "            if predict == None: continue\n",
    "                \n",
    "            pred_label, _, score = predict\n",
    "            if score < base_recognition_confidence:\n",
    "                pred_label = 'Unknown'\n",
    "                score = 0\n",
    "            \n",
    "            # draw face border\n",
    "            face_pos = face['box']\n",
    "            cv2.rectangle(frame, (face_pos[0], face_pos[1]), (face_pos[0]+face_pos[2],\n",
    "                                                            face_pos[1]+face_pos[3]), (0, 255, 0), 3)                \n",
    "            \n",
    "            # output prediction on image\n",
    "            text = f'{pred_label}'\n",
    "            if pred_label != 'Unknown': text += f', score:{int(score)}'\n",
    "            \n",
    "            cv2.putText(frame, text, (face_pos[0], face_pos[1]), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # set video frame rate and show frame\n",
    "    if time_elapsed > 1./frame_rate:\n",
    "        prev = time.time()\n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
